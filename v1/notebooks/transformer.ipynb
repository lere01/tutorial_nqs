{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import math\n",
    "import optax\n",
    "import numpy as np\n",
    "import flax.linen as nn\n",
    "from jax import nn as jnn\n",
    "from jax import numpy as jnp\n",
    "import jax.tree_util as tree_util   \n",
    "from functools import partial, lru_cache\n",
    "from dataclasses import dataclass, KW_ONLY\n",
    "from jax import random, jit, vmap, grad, value_and_grad, lax\n",
    "from typing import NamedTuple, Dict, List, Tuple, Any, Callable, Optional, Union\n",
    "\n",
    "from flax.linen import MultiHeadAttention, make_causal_mask, Embed\n",
    "from flax.linen.initializers import xavier_uniform, zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    d_model : int         \t# Hidden dimensionality of the input.\t\n",
    "    max_len : int = 16  \t# Maximum length of a sequence to expect.\n",
    "\n",
    "    def setup(self):\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = np.zeros((self.max_len, self.d_model))\n",
    "        position = np.arange(0, self.max_len, dtype=np.float32)[:,None]\n",
    "        div_term = np.exp(np.arange(0, self.d_model, 2) * (-math.log(10000.0) / self.d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        pe = pe[None]\n",
    "        self.pe = jax.device_put(pe)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x + self.pe[:, :x.shape[1]]\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NQS(nn.Module):\n",
    "    seq_len: int\n",
    "    num_layers: int\n",
    "    dropout_prob: float\n",
    "    \n",
    "    _: KW_ONLY\n",
    "\t# embedding parameters\n",
    "    embed_dim: int = 32\n",
    "    num_embeddings: int\t= 2\n",
    "    \n",
    "\n",
    "    def setup(self):\n",
    "        self.layers = [\n",
    "            MultiHeadAttention(\n",
    "                num_heads=8,\n",
    "\t\t\t\tkernel_init=xavier_uniform(),\n",
    "                dropout_rate=self.dropout_prob,\n",
    "\t\t\t) \n",
    "            \tfor _ in range(self.num_layers)\n",
    "        ]\n",
    "        self.embedding = Embed(num_embeddings=self.num_embeddings, features=self.embed_dim)\n",
    "        self.pos_encoder = PositionalEncoder(d_model=self.embed_dim, max_len=self.seq_len)\n",
    "        \n",
    "\t\t# set the dimension of the amplitude head to the num_embeddings\n",
    "        self.amplitude_head = nn.Dense(features=self.num_embeddings)\n",
    "\n",
    "    def __call__(self, x, mask=None, train=True):\n",
    "        # Setup mask\n",
    "        mask = make_causal_mask(x) if mask is None else mask\n",
    "\n",
    "        # Step: Positional encoding + input embedding\n",
    "        # x = jnp.squeeze(x, axis=-1)\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Step: Get logits from the transformer encoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask, deterministic=not train)\n",
    "        \n",
    "\t\t# Step: Apply Log-softmax to get probabilities\n",
    "        x = self.amplitude_head(x)\n",
    "        x = nn.activation.log_softmax(x, axis=-1)\n",
    "        \n",
    "\t\t# Step: Return the probabilities\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask=None, train=True):\n",
    "        # A function to return the attention maps within the model for a single application used for visualization purpose later\n",
    "        attention_maps = []\n",
    "        for l in self.layers:\n",
    "            _, attn_map = l.self_attn(x, mask=mask)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = l(x, mask=mask, train=train)\n",
    "        return attention_maps\n",
    "    \n",
    "    @staticmethod\n",
    "    def _generate_mask(sz):\n",
    "        mask = jnp.triu(jnp.ones((sz, sz)), k=1).T\n",
    "        mask = jnp.where(mask == 0, -jnp.inf, 0.0)\n",
    "        # mask = jnp.triu(jnp.ones((sz, sz)), k=1)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=128, typed=False)\n",
    "def get_all_interactions_jax(n: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Get all to all interactions from a n by n lattice using the euclidean distances.\n",
    "    Assume a unit distance (1) between nearest neighbours\n",
    "\n",
    "    Parameters\n",
    "    ---\n",
    "    n: integer representing a side of the square\n",
    "\n",
    "    Output\n",
    "    ---\n",
    "    tuple[unique_pairs, multipliers]\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a grid of coordinates\n",
    "    x, y = jnp.meshgrid(jnp.arange(n), jnp.arange(n))\n",
    "    coordinates = jnp.stack([x.flatten(), y.flatten()], axis=1)\n",
    "\n",
    "    # Calculate distances between all unique pairs\n",
    "    num_points = coordinates.shape[0]\n",
    "    distances = jnp.sqrt(\n",
    "        jnp.sum((coordinates[:, None, :] - coordinates[None, :, :]) ** 2, axis=-1)\n",
    "    )\n",
    "\n",
    "    # Mask to select only unique pairs\n",
    "    mask = jnp.triu(jnp.ones((num_points, num_points), dtype=bool), k=1)\n",
    "\n",
    "    # Extract unique pairs, distances, and calculate multipliers\n",
    "    unique_pairs = jnp.argwhere(mask)\n",
    "    unique_distances = distances[mask]\n",
    "    multipliers = 1 / unique_distances ** 6\n",
    "\n",
    "    return unique_pairs, multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VMC:\n",
    "    nsamples: int\n",
    "    n: int\n",
    "    learning_rate: float\n",
    "    num_epochs: int\n",
    "    output_dim: int\n",
    "    sequence_length: int\n",
    "    num_hidden_units: int\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.pairs, self.multipliers = get_all_interactions_jax(self.n)\n",
    "        self.Omega = 1.0\n",
    "        self.delta = 1.0\n",
    "\n",
    "    \n",
    "    \n",
    "    def sample(self, key, params, model) -> List[Union[float, Tuple[float, ...]]]:\n",
    "        sample_key, dropout_key = random.split(key)\n",
    "        samples = jnp.zeros((self.nsamples, 1), dtype=jnp.int32)\n",
    "        sample_keys = jax.random.split(sample_key, self.sequence_length)\n",
    "        dropout_keys = jax.random.split(dropout_key, self.sequence_length)\n",
    "\n",
    "        for i in range(self.sequence_length):\n",
    "            log_prob = model.apply({'params': params}, samples, train=True, rngs={'dropout': dropout_keys[i]})\n",
    "\n",
    "\t\t\t# take the last log-probability\n",
    "            log_prob = log_prob[:, -1, :]\n",
    "\n",
    "\t\t\t# sample from the log-probabilities\n",
    "            sample = random.categorical(sample_keys[i], log_prob)\n",
    "            sample = jnp.expand_dims(sample, axis=1)\n",
    "\n",
    "            samples = sample if i == 0 else jnp.concatenate([samples, sample], axis=1)\n",
    "\n",
    "        return samples\n",
    "    \n",
    "    # def sample(self, key, params, model) -> List[Union[float, Tuple[float, ...]]]:\n",
    "    #     sample_key, dropout_key = random.split(key)\n",
    "    #     samples = jnp.zeros((self.nsamples, 1), dtype=jnp.int32)\n",
    "    #     sample_keys = jax.random.split(sample_key, self.sequence_length)\n",
    "    #     dropout_keys = jax.random.split(dropout_key, self.sequence_length)\n",
    "        \n",
    "        \n",
    "    #     def step(i, state):\n",
    "    #         # network_input = lax.dynamic_slice(operand=state, start_indices=(0, 0), slice_sizes=(self.nsamples, i+1))\n",
    "    #         network_input, samples = state\n",
    "    #         log_prob = model.apply({'params': params}, network_input, train=True, rngs={'dropout': dropout_keys[i]})\n",
    "    #         log_prob = log_prob[:, -1, :]\n",
    "    #         sample = random.categorical(sample_keys[i], log_prob)\n",
    "    #         sample = jnp.expand_dims(sample, axis=1)\n",
    "    #         samples = lax.dynamic_update_slice(operand=samples, update=sample, start_indices=(0, i))\n",
    "    #         network_input = jnp.concatenate([network_input, sample], axis=1)\n",
    "    #         return network_input, state\n",
    "        \n",
    "    #     initial_samples = jnp.zeros((self.nsamples, self.sequence_length), dtype=jnp.int32)\n",
    "    #     initial_input = jnp.zeros((self.nsamples, 1), dtype=jnp.int32)\n",
    "    #     samples = lax.fori_loop(0, self.sequence_length, step, (initial_input, initial_samples))\n",
    "        \n",
    "    #     return samples\n",
    "    \n",
    "\n",
    "    def logpsi(self, samples: List[Union[float, Tuple[float, ...]]], params, model, dropout_key) -> List[float]:\n",
    "        ss = (0, self.sequence_length - 1)\n",
    "        nsamples = samples.shape[0]\n",
    "        data   = samples[:, ss[0]:ss[1]]\n",
    "        inputs = jnp.concatenate([jnp.zeros((nsamples, 1), dtype=jnp.int32), data], axis = 1)\n",
    "\n",
    "        log_probs = model.apply({'params': params}, inputs, train=False, rngs={'dropout': dropout_key})\n",
    "\n",
    "        logP   = jnp.sum(jnp.multiply(log_probs, jnn.one_hot(samples, self.output_dim)), axis=2)\n",
    "        logP = 0.5 * jnp.sum(logP, axis=1)\n",
    "        return logP\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def get_loss(self, params, rng_key, model):\n",
    "        def l2_loss(x, alpha):\n",
    "            return alpha * (x ** 2).mean()\n",
    "\n",
    "        @jit\n",
    "        def all_reg():\n",
    "            return sum(\n",
    "                l2_loss(w, alpha=0.001) for w in tree_util.tree_leaves(params[\"params\"])\n",
    "            )\n",
    "        \n",
    "        s_key, d_key, d_key2 = random.split(rng_key, 3)\n",
    "\n",
    "        samples = self.sample(s_key, params, model)\n",
    "        log_psi = self.logpsi(samples, params, model, d_key)\n",
    "        e_loc = self.local_energy(samples, params, model, log_psi, d_key2)\n",
    "        e_o = e_loc.mean()\n",
    "\n",
    "        # We expand the equation in the text above\n",
    "        first_term = 2 * jnp.multiply(log_psi, e_loc)\n",
    "        second_term = 2 * jnp.multiply(e_o, log_psi)\n",
    "\n",
    "        # l2_reg = all_reg()\n",
    "\n",
    "        loss = jnp.mean(first_term - second_term)\n",
    "        # loss = l2_reg(params) + loss\n",
    "        # loss += l2_reg\n",
    "        return loss, e_loc\n",
    "    \n",
    "\n",
    "    def train(self, rng_key, params, model):\n",
    "        rng_key = random.PRNGKey(0)\n",
    "        \n",
    "        optimizer = optax.adam(learning_rate=self.learning_rate)\n",
    "        opt_state = optimizer.init(params)\n",
    "\n",
    "        loss_fn = self.get_loss\n",
    "    \n",
    "\n",
    "        @partial(jit, static_argnums=(3,))\n",
    "        def step(params, rng_key, opt_state, get_loss=loss_fn):\n",
    "            rng_key, new_key = random.split(rng_key)\n",
    "\n",
    "            value, grads = value_and_grad(get_loss, has_aux=True)(params, rng_key, model)\n",
    "            updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "            return new_key, params, opt_state, value\n",
    "\n",
    "        energies = []\n",
    "        for i in range(self.num_epochs):\n",
    "            rng_key, params, opt_state, (loss, eloc) = step(params, rng_key, opt_state)\n",
    "            energies.append(eloc)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f'step {i}, loss: {loss}')\n",
    "\n",
    "        return energies\n",
    "    \n",
    "    \n",
    "    \n",
    "    def local_energy(self, samples, params, model, log_psi, dropout_key) -> List[float]:\n",
    "        output = jnp.zeros((samples.shape[0]), dtype=jnp.float32)\n",
    "\n",
    "        def step_fn_chemical(i, state):\n",
    "            s, output = state\n",
    "            output += - self.delta * s[:, i]\n",
    "            return s, output\n",
    "\n",
    "        def step_fn_intr(i, state):\n",
    "            samples, pairs, multipliers, output = state\n",
    "            output += multipliers[i] * samples[:, pairs[i, 0]] * samples[:, pairs[i, 1]]\n",
    "            return samples, pairs, multipliers, output\n",
    "\n",
    "\n",
    "        def step_fn_transverse(i, state):\n",
    "            s, output = state\n",
    "            flipped_state = s.at[:, i].set(1 - s[:, i])\n",
    "            flipped_logpsi = self.logpsi(flipped_state, params, model, dropout_key)\n",
    "            output += - self.Omega * jnp.exp(flipped_logpsi - log_psi)\n",
    "            return s, output\n",
    "\n",
    "\n",
    "    \t# Interaction Term\n",
    "        _, _, _, interaction_term = lax.fori_loop(0, 120, step_fn_intr, (samples, self.pairs, self.multipliers, output))\n",
    "        # Off Diagonal Term\n",
    "        _, transverse_field = lax.fori_loop(0, 16, step_fn_transverse, (samples, output))\n",
    "        # _, transverse_field = lax.fori_loop(0, 16, step_fn_transverse, (samples, output))\n",
    "        # Occupancy Term\n",
    "        _, chemical_potential = lax.fori_loop(0, 16, step_fn_chemical, (samples, output))\n",
    "\n",
    "        # Total energy\n",
    "        loc_e = transverse_field + chemical_potential + interaction_term\n",
    "\t\t\n",
    "        return chemical_potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VMCConfig(NamedTuple):\n",
    "    nsamples: int = 500\n",
    "    n: int = 4\n",
    "    learning_rate: float = 0.01\n",
    "    num_epochs: int = 500\n",
    "    output_dim: int = 2\n",
    "    sequence_length: int = 16\n",
    "    num_hidden_units: int = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1,16)\n",
    "nqs = NQS(seq_len=16, num_layers=8, dropout_prob=0.5)\n",
    "params = nqs.init(jax.random.PRNGKey(0), jnp.ones(input_shape, dtype=jnp.int32))['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 1.4691606760025024\n",
      "step 100, loss: -750928.9375\n",
      "step 200, loss: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m vmc \u001b[38;5;241m=\u001b[39m VMC(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_asdict())\n\u001b[0;32m      3\u001b[0m rng_key \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m densities \u001b[38;5;241m=\u001b[39m \u001b[43mvmc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnqs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 127\u001b[0m, in \u001b[0;36mVMC.train\u001b[1;34m(self, rng_key, params, model)\u001b[0m\n\u001b[0;32m    125\u001b[0m energies \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[1;32m--> 127\u001b[0m     rng_key, params, opt_state, (loss, eloc) \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     energies\u001b[38;5;241m.\u001b[39mappend(eloc)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = VMCConfig()\n",
    "vmc = VMC(**config._asdict())\n",
    "rng_key = random.PRNGKey(0)\n",
    "densities = vmc.train(rng_key, params, nqs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer_school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
