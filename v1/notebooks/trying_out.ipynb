{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summer School"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from src.attention import scaled_dot_product, MultiheadAttention, EncoderBlock, TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_rng = random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values=Array([[ 0.376226  , -0.1465618 ],\n",
      "       [-0.42778558, -0.5989566 ],\n",
      "       [ 0.43624768, -0.11678301]], dtype=float32)\n",
      "attention=Array([[0.27963293, 0.54049295, 0.17987415],\n",
      "       [0.22194658, 0.06706189, 0.7109916 ],\n",
      "       [0.27977085, 0.5837308 , 0.13649832]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "main_rng, rand1 = random.split(main_rng)\n",
    "qkv = random.normal(rand1, (3, seq_len, d_k))\n",
    "q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "\n",
    "print(f\"{values=}\\n{attention=}\")\n",
    "del rand1, qkv, q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape=(3, 16, 128)\n",
      "attn.shape=(3, 4, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "main_rng, x_rng = random.split(main_rng)\n",
    "x = random.normal(x_rng, (3, 16, 128))\n",
    "mh_attn = MultiheadAttention(embed_dim=128, num_heads=4)\n",
    "\n",
    "main_rng, init_rng = random.split(main_rng)\n",
    "params = mh_attn.init(init_rng, x)['params']\n",
    "out, attn = mh_attn.apply({'params': params}, x)\n",
    "\n",
    "print(f\"{out.shape=}\\n{attn.shape=}\")\n",
    "del mh_attn, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (3, 16, 128)\n"
     ]
    }
   ],
   "source": [
    "main_rng, x_rng = random.split(main_rng)\n",
    "x = random.normal(x_rng, (3, 16, 128))\n",
    "\n",
    "encoder_block = EncoderBlock(input_dim=128, num_heads=4, dim_feedforward=512, dropout_prob=0.1)\n",
    "main_rng, init_rng, dropout_init_rng = random.split(main_rng, 3)\n",
    "params = encoder_block.init({'params': init_rng, 'dropout': dropout_init_rng}, x, train=True)['params']\n",
    "\n",
    "main_rng, dropout_apply_rng = random.split(main_rng)\n",
    "output = encoder_block.apply({'params': params}, x, train=True, rngs={'dropout': dropout_apply_rng})\n",
    "\n",
    "print('Output', output.shape)\n",
    "del encoder_block, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (3, 16, 128)\n",
      "Attention maps 5 (3, 4, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "main_rng, x_rng = random.split(main_rng)\n",
    "x = random.normal(x_rng, (3, 16, 128))\n",
    "\n",
    "trans_enc = TransformerEncoder(\n",
    "    num_layers = 5,\n",
    "    input_dim=128,\n",
    "    num_heads=4,\n",
    "    dim_feedforward=256,\n",
    "    dropout_prob=0.15\n",
    ")\n",
    "\n",
    "main_rng, init_rng, dropout_init_rng = random.split(main_rng, 3)\n",
    "params = trans_enc.init({'params': init_rng, 'dropout': dropout_init_rng}, x, train=True)['params']\n",
    "\n",
    "main_rng, dropout_apply_rng = random.split(main_rng)\n",
    "binded_mod = trans_enc.bind({'params': params}, rngs={'dropout': dropout_apply_rng})\n",
    "output = binded_mod(x, train=True)\n",
    "print('Output', output.shape)\n",
    "\n",
    "attention_maps = binded_mod.get_attention_maps(x, train=True)\n",
    "print('Attention maps', len(attention_maps), attention_maps[0].shape)\n",
    "\n",
    "del trans_enc, binded_mod, params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
